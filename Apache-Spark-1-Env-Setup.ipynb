{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgs8n2EdQWtw7b8wFWh6qs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CRforty6/Apache-Spark-CodeLabs/blob/main/CodeLab1_ApacheSpark1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî•CodeLab 1Ô∏è: Apache Spark for Large-scale Data Processing #ApacheSpark #Part1üî•\n",
        "\n",
        "Apache Spark is a powerful distributed computing framework that's great for processing large datasets. <br />\n",
        "In this CodeLab, I'll guide you through the following (with Google Colab):\n",
        "\n",
        "‚õèÔ∏è Set up Apache Spark environment (and SparkSession) <br/>\n",
        "üöÄ Run a simple Spark application (with data load + operations) <br/>\n",
        "üî≠ Launch the Spark Web UI (to monitor applications)\n",
        "\n",
        "<br />\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "8lgdK8e4z-W8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚õèÔ∏èSet up Apache Spark environment (and SparkSession)\n",
        "\n",
        "We will be using the following setup approach:\n",
        "1. Java 8, 11 or 17\n",
        "2. Python 3.8+ (Spark supports Java, Scala 2.12 or 2.13, Python 3.8+ and R 3.5+)\n",
        "3. Spark 3.5.1 (Feb 2024 release)\n",
        "4. Findspark python package\n",
        "\n",
        "*Alternatively, skip 3 & 4 above and install PySpark python package for just python support.*"
      ],
      "metadata": {
        "id": "QZu11QZCrV0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£: Install Apache Spark and its dependencies\n",
        "\n",
        "Let's install Apache Spark and its dependencies. <br />\n"
      ],
      "metadata": {
        "id": "wTMUhuKKmniE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Java installation on Colab. Colab already has Java available\n",
        "!java -version"
      ],
      "metadata": {
        "id": "uTdo2Tc9HBJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Python installation on Colab. Colab already has Python available too\n",
        "!python --version"
      ],
      "metadata": {
        "id": "oH8hi-yPMLOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Spark 3.5.1\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "# Unzip Spark\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install findspark to use spark with python\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "kZqhTuZdmo4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the environment variables to point to the Java and Spark install locations\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"spark-3.5.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "qNqd-L5SlQ81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£: Start a SparkSession\n",
        "Let's create a SparkSession and print its version."
      ],
      "metadata": {
        "id": "PgdC4qKcpAMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Codelab1\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "zJG_SdCgqydo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check SparkSession Setup on Colab\n",
        "spark"
      ],
      "metadata": {
        "id": "AP96S6urpB1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄRun a simple Spark application (with data load + operations)\n",
        "\n",
        "We'll load data in a few different ways and perform some basic operations: <br />\n",
        "* Manual data entry\n",
        "* Data file in directory\n",
        "* External data file"
      ],
      "metadata": {
        "id": "2WXSxySy62Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£: Data load and basic operations - manual data entry\n",
        "\n",
        "create a DataFrame and perform a basic operation on it - with manual data entry."
      ],
      "metadata": {
        "id": "9M4Ws9ULplPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with manually input data\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 27)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# Filter data\n",
        "df_filtered = df.filter(df.Age > 30)\n",
        "\n",
        "# Show the result\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "id": "6YOOAG0q9lvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£: Data load and basic operations - data file in directory\n",
        "\n",
        "create a DataFrame and perform basic operations on it - with csv data file from Google Colab's 'sample_data' folder."
      ],
      "metadata": {
        "id": "U-jKHT3qBYcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from file in directory\n",
        "df = spark.read.csv(\"sample_data/california_housing_test.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# View the first few rows\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "8xS7-5XkpqQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the table schema\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "Lacod6oqu-Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform some count operations\n",
        "print(\"Number of rows:\", df.count())\n",
        "print(\"Number of columns:\", len(df.columns))"
      ],
      "metadata": {
        "id": "l4gV4dzGu_Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£: Data load and basic operations - external data file\n",
        "\n",
        "create a DataFrame and perform basic operations on it - with csv data file stored in a Github repository."
      ],
      "metadata": {
        "id": "_FDFaBOvp0yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from external data file\n",
        "from pyspark import SparkFiles\n",
        "url_ext = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/nba-elo/nbaallelo.csv' # sample sports data - NBA\n",
        "\n",
        "spark.sparkContext.addFile(url_ext)\n",
        "df = spark.read.csv(SparkFiles.get(\"nbaallelo.csv\"),inferSchema=True, header=True)\n",
        "\n",
        "# View the first few rows\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "qeuxLMfavo_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group data and calculate averages\n",
        "avg_points = df.groupBy(\"team_id\").agg({\"pts\": \"avg\"}).withColumnRenamed(\"avg(pts)\", \"avg_points\")\n",
        "\n",
        "# Show the result\n",
        "avg_points.show()"
      ],
      "metadata": {
        "id": "zcdNI8N-p7P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spark sql function to perform column operations\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Filter data and show result\n",
        "lakers_data = df.filter(col(\"fran_id\") == \"Lakers\")\n",
        "\n",
        "# Show the result - first few rows\n",
        "lakers_data.show(5)"
      ],
      "metadata": {
        "id": "83y3XLTaHR7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî≠Launch the Spark Web UI to Monitor Applications\n",
        "\n",
        "The Spark Web UI provides detailed information about running Spark applications, including job progress, resource utilization, and DAG visualization.\n",
        "\n",
        "We'll start a tunnel to access SparkUI on Google Colab.\n",
        "1. Sign-in to https://dashboard.ngrok.com/get-started/your-authtoken and copy your AuthToken\n",
        "2. Run the code below and enter your AuthToken\n",
        "3. SparkUI will be accessible on the provided *temporary public URL*"
      ],
      "metadata": {
        "id": "cEnLCrDMLGgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a tunnel to access SparkUI on Google Colab\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "\n",
        "print(\"Enter your authtoken, which can be copied \"\n",
        "\"from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "\n",
        "ui_port = 4040\n",
        "public_url = ngrok.connect(ui_port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{ui_port}\\\"\")"
      ],
      "metadata": {
        "id": "3w8Ewmtgff5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚õ≥Summary\n",
        "\n",
        "**What we covered**\n",
        "* Set up Apache Spark environment and SparkSession\n",
        "* Run a simple Spark application to test the setup\n",
        "* Launch the Spark web UI to monitor applications\n",
        "\n",
        "**Additional Resources:**\n",
        "* [Apache Spark Documentation](https://spark.apache.org/docs/latest/): In-depth knowledge and tutorials.\n",
        "* Online courses and tutorials for advanced Apache Spark learning."
      ],
      "metadata": {
        "id": "bbSibbu0lkn7"
      }
    }
  ]
}